{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnalysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN2opiK7CFqEkdIQ8inChrO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PavanReddy28/CRuX/blob/main/SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH_FiwpiB_Ff"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zOhpSkBB28-"
      },
      "source": [
        "## Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxg7HM77hna1"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGsj9S8whpfR",
        "outputId": "a6ea0974-7628-4fc8-950b-d11ada855a52"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tRdJPRHSSpq"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import time\r\n",
        "\r\n",
        "import re\r\n",
        "from nltk import WordNetLemmatizer\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ShputfAB2Pq"
      },
      "source": [
        "## Importing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xWI8OYUZNUc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "175279e0-014f-4c48-d512-bb7ac0cecfbf"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dom9XH0eOH5C"
      },
      "source": [
        "### Different datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHGced-mLJRb"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiXaPmG1LOnW"
      },
      "source": [
        "train.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT9oRGUBLZUK"
      },
      "source": [
        "test = pd.read_csv(\"/gdrive/My Drive/Inductions/test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuV32NmKMQHq"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFv3x2J5MRfr"
      },
      "source": [
        "train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7_OYb5cMdTa"
      },
      "source": [
        "test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSBYZ3QKO0KS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWSXZbVEO1-R"
      },
      "source": [
        "#hi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm1siqftO49Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3vBJdkzJmSI"
      },
      "source": [
        "train =  pd.read_csv(\"/gdrive/My Drive/Inductions/train.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7b37UP2SKW4"
      },
      "source": [
        "### Sentiment140 Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll0HW-mjND4F"
      },
      "source": [
        "Using the dataset **Sentiment140** contatining **1,600,000** tweets extracted using twitter api. \r\n",
        "<br>The tweets are annoted as '0 : Negative', '2 : Neutral' and '4 : Positive'.\r\n",
        "<br>I have only used the positive and negative tweets to train my model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1NTvgd-Mgsm"
      },
      "source": [
        "twitter_dataset = pd.read_csv('/gdrive/My Drive/Inductions/training.1600000.processed.noemoticon.csv', encoding=\"ISO-8859-1\", names=[\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "Ortz5Y-cPt2L",
        "outputId": "6940ab7b-afb8-4451-8296-0f45b380cbf9"
      },
      "source": [
        "twitter_dataset.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>ids</th>\n",
              "      <th>date</th>\n",
              "      <th>flag</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment  ...                                               text\n",
              "0          0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1          0  ...  is upset that he can't update his Facebook by ...\n",
              "2          0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "3          0  ...    my whole body feels itchy and like its on fire \n",
              "4          0  ...  @nationwideclass no, it's not behaving at all....\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J442IpvYXwVf"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OroosyT-PzsA"
      },
      "source": [
        "sentiment, text = twitter_dataset['sentiment'], twitter_dataset['text']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA-HZD_cX1HT"
      },
      "source": [
        "### **Sentiment**\r\n",
        "\r\n",
        "Replacing 4's to 1's (Representation of Positive Data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxf6J3rTPzl-",
        "outputId": "a2d8d931-ecf9-46d6-8648-a7a4ff475d36"
      },
      "source": [
        "sentiment.value_counts()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    800000\n",
              "0    800000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLkfUuUWV8V2"
      },
      "source": [
        "sentiment = sentiment.replace(4,1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WR69l6HjW0FK",
        "outputId": "5f619fbf-60b7-409c-d1b4-f6b93e4fc6da"
      },
      "source": [
        "sentiment.value_counts()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    800000\n",
              "0    800000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1RRs00rW86V"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwcAJypvYI5v"
      },
      "source": [
        "### **Text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5GwHuEuoLvs"
      },
      "source": [
        "Preprocessing of text data include:\r\n",
        "<ol type = \"1\">\r\n",
        "<li>Converting all the data to **Lower Case**</li>\r\n",
        "         <li>Replaceing **User ID's** (\"@colab\", etc.) with \"USER\"</li>\r\n",
        "         <li>Replacing **Emojis** to text representation (According to Emoji Dictionary).</li>\r\n",
        "         <li>Replacing **URLs** (starting with \"http\", \"www\", etc.) with \"URL\"</li>\r\n",
        "         <li>Removing **Non-Alphabets**</li>\r\n",
        "         <li>**Tokenization** : Splitting of the tweets to, list of words.</li>\r\n",
        "         <li>**Lemmetization** : Grouping together the different forms of a work so that they can be analyzed as a single word. Done using NLTK Library.\r\n",
        "</li>\r\n",
        "<li>Removing letters repeated more than 3 times.</li>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWsftnx-c4Gp"
      },
      "source": [
        "#Emojis and Stopwords taken from the internet.\r\n",
        "\r\n",
        "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \r\n",
        "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\r\n",
        "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \r\n",
        "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\r\n",
        "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\r\n",
        "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \r\n",
        "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\r\n",
        "\r\n",
        "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\r\n",
        "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\r\n",
        "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\r\n",
        "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \r\n",
        "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\r\n",
        "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\r\n",
        "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\r\n",
        "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\r\n",
        "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\r\n",
        "             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\r\n",
        "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\r\n",
        "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \r\n",
        "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\r\n",
        "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\r\n",
        "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\r\n",
        "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH85JF7IbsNB"
      },
      "source": [
        "def data_preprocess(textdata):\r\n",
        "\r\n",
        "  lemmatizer = WordNetLemmatizer()\r\n",
        "\r\n",
        "  Processed =[]\r\n",
        "\r\n",
        "  urlPattern = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\r\n",
        "  userPattern = '@[^\\s]+'\r\n",
        "  alphaPattern = r\"[^\\w]\"\r\n",
        "  sequenceFind = r\"(.)\\1\\1+\"\r\n",
        "  sequenceReplace = r\"\\1\\1\"\r\n",
        "\r\n",
        "  for tweet in textdata:\r\n",
        "\r\n",
        "    tweet = tweet.lower()\r\n",
        "\r\n",
        "    tweet = re.sub(urlPattern, 'URL ', tweet)\r\n",
        "    tweet = re.sub(userPattern, 'USER ', tweet)\r\n",
        "    tweet = re.sub(alphaPattern, ' ', tweet)\r\n",
        "    tweet = re.sub(sequenceFind, sequenceReplace, tweet)\r\n",
        "\r\n",
        "    for emoji in emojis.keys():\r\n",
        "      tweet = tweet.replace(emoji, \"EMOJI\"+emojis[emoji])\r\n",
        "\r\n",
        "    tweetWords = ''\r\n",
        "    for text in tweet.split():\r\n",
        "      if len(text)>1:\r\n",
        "        text = lemmatizer.lemmatize(text)\r\n",
        "        tweetWords += (text +' ')\r\n",
        "    \r\n",
        "    Processed.append(tweetWords)\r\n",
        "\r\n",
        "  return Processed\r\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDlk6dcNg0S9",
        "outputId": "c2a62ef2-30da-4274-bdf4-983aee5d3850"
      },
      "source": [
        "%%time\r\n",
        "text = data_preprocess(text)\r\n",
        "print('Data Processed.')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Processed.\n",
            "CPU times: user 1min 35s, sys: 978 ms, total: 1min 36s\n",
            "Wall time: 1min 36s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsBYzs-0PAMz"
      },
      "source": [
        "### Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buX2UqZ3hEXb"
      },
      "source": [
        "X_train, Y_train, X_test, Y_test = train_test_split(text, sentiment, test_size=0.05, random_state=0)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dNImZjQPEX4",
        "outputId": "171b2352-38c7-4921-c4c0-9e9b0245dc93"
      },
      "source": [
        "X_test[:10]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "391051     0\n",
              "197655     0\n",
              "905468     1\n",
              "1492339    1\n",
              "551346     0\n",
              "909061     1\n",
              "1091617    1\n",
              "1323317    1\n",
              "1249935    1\n",
              "87105      0\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-I6K2S2kLmi"
      },
      "source": [
        "### **TF-IDF Vectorizer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6H7Fm7-oSgl"
      },
      "source": [
        "The 'tfidfVectorizer()' method from the sklearn.feature_extraction package, helps convert a collection of raw documents to a matrix of TF-IDF features.\r\n",
        "\r\n",
        "It returns a list of features (here, words are the features) which have the most count.\r\n",
        "\r\n",
        "**Example**:\r\n",
        "<br>\r\n",
        "INPUT: corpus = ['This is the first document.','This document is the second document.','And this is the third one.','Is this the first document?',]\r\n",
        "<br><br>When the above list is passed through the vectorizer the below list is returned.\r\n",
        "<br><br>OUTPUT: ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJx_zd_Fj9Ww"
      },
      "source": [
        "Vectorizer = TfidfVectorizer(ngram_range=(1,2), lowercase=True, max_features=50000)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ud-hVNQSoupE",
        "outputId": "1382a427-5bb8-457e-d8b7-6c429b90b5fd"
      },
      "source": [
        "Vectorizer.fit(X_train)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=1.0, max_features=50000,\n",
              "                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
              "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFLBXQMHpN0O"
      },
      "source": [
        "X_train = Vectorizer.transform(X_train)\r\n",
        "X_test = Vectorizer.transform(Y_train)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1tSFu8TQ8nz",
        "outputId": "becac666-9072-4679-86f2-120e54653f96"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1520000, 50000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dU2pfjOWZQ7",
        "outputId": "add1770f-684c-495a-fb05-507cca7f386f"
      },
      "source": [
        "X_train[1]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x50000 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 14 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv7uwfgLSlFY"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKHOox-eSnnL"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO1_pYmnSm51"
      },
      "source": [
        "class NaiveBayesAlgo:\r\n",
        "\r\n",
        "  def __init__(self, train, test):\r\n",
        "\r\n",
        "\r\n",
        "  def fit():\r\n",
        "    \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DDbGi5aSmHB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}